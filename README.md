# ğŸ“š RAG-Based Interactive Learning with LLM

This project implements a **Retrieval-Augmented Generation (RAG)** system using a Large Language Model (LLM) to enable **interactive learning** from domain-specific content. Specifically, it uses lecture notes on **Pattern Recognition** from **FAU Erlangen-NÃ¼rnberg**, and stores them in a **Pinecone vector database** for efficient semantic search and retrieval.

## ğŸš€ Features

- âš¡ Retrieval-Augmented Generation using LLM
- ğŸ“˜ Semantic search over FAU's pattern recognition lecture notes
- ğŸ’¬ Interactive interface using Streamlit
- ğŸ§  Knowledge-aware response generation


ğŸ“‚ Data
- The project uses Pattern Recognition lecture notes from FAU Erlangen-NÃ¼rnberg, processed and embedded into a Pinecone vector store to enable semantic retrieval.

ğŸ“¦ Technologies Used
- ğŸ§  OpenAI or HuggingFace LLMs (configurable)
- ğŸ“ Pinecone for vector storage
- ğŸ› ï¸ Streamlit for front-end


ğŸ¤ Contributing
- We welcome contributions! Please feel free to submit pull requests, issues, or feature requests.


## ğŸ› ï¸ Setup Instructions

Follow the steps below to get the project running locally:

## 1. Creating a Virtual Environment: You can create a virtual environment using the following command:
   ```bash
   python -m venv myenv
   ```
   
## 2. Install Requirements: Once you have the requirements.txt file, you can install the dependencies listed in it using pip:
   ```bash
   pip install -r requirements.txt
   ```
   
## 3. Start the application using
   ```bash
   streamlit run stream_lit.py
   ```
